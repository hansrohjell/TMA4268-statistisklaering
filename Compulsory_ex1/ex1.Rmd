---
subtitle: "TMA4268 Statistical Learning V2021"
title: "Compulsory exercise 1: Group 5"
author: "Hans RÃ¸hjell Odland and Aksel Haugen Madslien"
date: "2/9/2021"
output: 
  pdf_document
editor_options: 
  chunk_output_type: inline
---
  

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=4, fig.height=3)
library(tidyverse)
library(ggplot2)
library(caret)
library(boot)
library(class)
library(MASS)

```

```{r rpackages,eval=FALSE,echo=FALSE}
install.packages("knitr") #probably already installed
install.packages("rmarkdown") #probably already installed
install.packages("ggplot2") #plotting with ggplot
install.packages("ggfortify")  
install.packages("MASS")
install.packages("class")
install.packages("pROC")
install.packages("plotROC")
install.packages('boot',dep=TRUE)
install.packages('topicmodels')
install.packages('formatR')
```





# Problem 1

## a)
 We consider $Y=f(\mathbf {x})+\varepsilon, \text{ where } \text{E}(\varepsilon)=0 \text{ and } \text{Var}(\varepsilon)=\sigma^2.$
 
We find the expected value for $\tilde{\beta}$ as 


\begin{align}
  E(\tilde{\beta}) &= E[(\mathbf{x}^T\mathbf{x}+\lambda \mathbf{I})^{-1}\mathbf{x}^T\mathbf{y}] \\
  &= (\mathbf{x}^T\mathbf{x}+\lambda \mathbf{I})^{-1} \mathbf{x}^T E[\mathbf{y}] \\
  &= (\mathbf{x}^T\mathbf{x}+\lambda \mathbf{I})^{-1} \mathbf{x}^T \mathbf{x} \beta + \lambda\mathbf{I}\beta - \lambda\mathbf{I}\beta \\
  &= (\mathbf{x}^T\mathbf{x}+\lambda \mathbf{I})^{-1} (-\lambda\mathbf{I})\beta + \mathbf{I}\beta \\
  &= \beta - \lambda(\mathbf{x}^T\mathbf{x}+\lambda \mathbf{I})\beta
\end{align}

## b)
 We let $\widetilde{f}(\mathbf{x}_0)=\mathbf{x}_0^T \widetilde{\boldsymbol{\beta}}$ The variance for $\widetilde{f}(\mathbf{x}_0)$ then becomes
 
 \begin{align}
 E[\widetilde{f}(\mathbf{x}_0)]&=E[\mathbf{x}_0^T \widetilde{\boldsymbol{\beta}}]\\ 
 &= \mathbf{x}_0^T E[\widetilde{\boldsymbol{\beta}}] \\
 &= \mathbf{x}_0^T(\beta - \lambda(\mathbf{x}^T\mathbf{x}+\lambda \mathbf{I})\beta)
 \end{align}
 
 
 For the variation we get 
 
 \begin{align}
 Var[\widetilde{f}(\mathbf{x}_0)] &= \mathbf{x}_0^T Var[\widetilde{\boldsymbol{\beta}}]\mathbf{x} \\
 &= \mathbf{x}_0^T(\beta - \lambda(\mathbf{x}^T\mathbf{x}+\lambda \mathbf{I})\beta)\mathbf{x}
 \end{align}
 
 
## c)

\begin{align}
\text{E}[(y_0-\widetilde{f}(\mathbf{x}_0))^2]&=[\text{E}(\widetilde{f}(\mathbf{x}_0)-f(\mathbf{x}_0))]^2+\text{Var}(\widetilde{f}(\mathbf{x}_0) \\
\end{align}


## d) 

```{r, echo=TRUE, eval=TRUE}
id <- "1X_8OKcoYbng1XvYFDirxjEWr7LtpNr1m" # google file ID
values <- dget(sprintf("https://docs.google.com/uc?id=%s&export=download", id))
X = values$X
dim(X)
x0 = values$x0
dim(x0)
beta=values$beta
dim(beta)
sigma=values$sigma
sigma
```



```{r}

bias = function(lambda, X, x0, beta) {
    p = ncol(X)
    value = -lambda*(t(X)%*%X%*%beta + lambda*beta)
    return(value)
}
lambdas = seq(0, 2, length.out = 500)
BIAS = rep(NA, length(lambdas))
for (i in 1:length(lambdas)) BIAS[i] = bias(lambdas[i], X, x0, beta)
dfBias = data.frame(lambdas = lambdas, bias = BIAS)
ggplot(dfBias, aes(x = lambdas, y = bias)) + geom_line(color = "red") + xlab(expression(lambda)) + 
    ylab(expression(bias^2))
```

 

# Problem 2

## a) 

```{r}
id <- "1yYlEl5gYY3BEtJ4d7KWaFGIOEweJIn__" # google file ID

d.corona <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id),header=T)

```

Number of deceased and non-deceased [Non-deceased = 0, deceased = 1]
```{r}
table(d.corona$deceased)
```

The number of males and females for each country
```{r}
table(Country = d.corona$country, sex = d.corona$sex)
```


The number of deceased and non-deceased for each sex [Non-deceased = 0, deceased = 1]
```{r}
table(sex = d.corona$sex, deceased = d.corona$deceased)
```

The number of deceased and non-deceased in France, separated for each sex [Non-deceased = 0, deceased = 1]

```{r}
francedf <- subset(d.corona, country == "France")
table(francedf$sex, francedf$deceased)
```




## b) 

### i)

The covariates sex, country and age is included to inspect te probability of dying of covid at age 75 in Korea. The function is fitted and summarized to get the coefficients. To get the age of 75 we had to multiply the covariate for age with 75.
```{r}
fit <- lm(as.numeric(deceased) ~ sex + country + age, data=d.corona)
summary(fit)$coef # show results

deceasedmale <- fit$coefficients[6]*75 + fit$coefficients[2] + fit$coefficients[5]

 


```

The probability of dying of Covid-19 for a male at age 75 in Korea is found to be ```r round(deceasedmale*100, 3)```%.


### ii)

Do males have higher probability to die than females?

```{r}

fit <- glm(deceased ~ sex, data = d.corona, family = binomial)
summary(fit)$coef


```

The estimate readings for men dying of corona is positive. At the same time the p-value for age is significant. This means that we can conclude that men have a higher probability of dying of corona than women.


### iii)

```{r}

fit <- glm(deceased ~ country, data = d.corona, family = 'binomial')
summary(fit)$coef

```
From these readings we can conclude that there is not enough evidence to say that there is a higher risk of dying of corona in Indonesia than in France, since the p-value is not significant. For Japan and Korea the p-value is much more significant and less than the alpha value of 5%, and also has a negative estimate, which means that there is a higher risk of dying of corona in Japan and Korea than in France.

### iv)
A person is 10 years older than another person. The probability of dying is linear in terms of age because of the logic regression, so we can see the probability of a person dying at an age of 65 and an age of 75 from task i and see that there is an increase of risk to die in case of higher age.

```{r}

deceasedmale75 <- fit$coefficients[6]*75 + fit$coefficients[2] + fit$coefficients[5]
deceasedmale65 <- fit$coefficients[6]*65 + fit$coefficients[2] + fit$coefficients[5]

diff <- (deceasedmale75 - deceasedmale65)*100
``` 

This gives an age difference in `r round(diff, 3) `% 



## c)


### i)

```{r}
fit <- glm(deceased ~ age*sex, data = d.corona, family = 'binomial')
summary(fit)$coef

``` 

Here we see that the `age:sexmale` coefficients has a positive estimate, but doesn't have a significant p-value. Age has a slightly lower p-value, and we can see that age is not a greater risk factor for males than for females.


### ii)

We fitted the function to find the relation with country and age.
```{r}
fit <- glm(deceased ~ age*country, data = d.corona, family = 'binomial')
summary(fit)$coef


```
We found that the coefficient for the `age:countryindonesia` interaction is negative, which means that Indonesia is lower than it is for France. The p-value is slightly significant, which gives a low but greater risk factor for the Indonesian population than for the French.


## d)
First we fitted the dataset with all the covariates
```{r}

fit <- glm(deceased ~ . ,data = d.corona , family = 'binomial')
summary(fit)$coef
```


Then we used the three predictor variables age, sex and country for LDA to print the confusion table for LDA 
```{r}
table(predict = predict(lda(deceased ~ age + sex + country, data = d.corona))$class, true = d.corona$deceased)
```

and did the same for the counfusion matrix for QDA

```{r}
table(predict = predict(qda(deceased ~ age + sex + country, data = d.corona))$class, true = d.corona$deceased)

```

The answers will then be
FALSE, TRUE, TRUE, FALSE


### Problem 3

## a)

```{r, eval=T}
#read file
id <- "1i1cQPeoLLC_FyAH0nnqCnnrSBpn05_hO" # google file ID
diab <- dget(sprintf("https://docs.google.com/uc?id=%s&export=download", id))
t = MASS::Pima.tr2
train = diab$ctrain
test = diab$ctest
```

```{r}
logReg = glm(diabetes~., data = train, family = "binomial")
summary(logReg)
```
### i)
prove that logit is linear
By denoting the term $\beta_0 + \beta_1x_{i1} + \beta_2 x_{i2} + \dots + \beta_7 x_{i7}$ as $\beta_{sum}$, we can derive that


\begin{align}
\text{logit}(p_i) = \log(\frac{p_i}{1-p_i})
&= \log(\frac{\frac{e^{\beta_{sum}}}{ 1+ 
e^{\beta_{sum}}}}{1-\frac{e^{\beta_{sum}}}{ 1+ 
e^{\beta_{sum}}}}) \\
&= \log(\frac{\frac{e^{\beta_{sum}}}{ 1+ 
e^{\beta_{sum}}}}{\frac{1+e^{\beta_{sum}}}{ 1+ 
e^{\beta_{sum}}}-\frac{e^{\beta_{sum}}}{ 1+ 
e^{\beta_{sum}}}}) \\
&= \log(\frac{\frac{e^{\beta_{sum}}}{ 1+ 
e^{\beta_{sum}}}}{\frac{1+e^{\beta_{sum}} - e^{\beta_{sum}}}{ 1+ 
e^{\beta_{sum}}}}) \\
&= \log(\frac{e^{\beta_{sum}}}{1+e^{\beta_{sum}} - e^{\beta_{sum}}})
&= \beta_{sum}
\end{align}

As we know, $$\beta_{sum}=\beta_0 + \beta_1x_{i1} + \beta_2 x_{i2} + \dots + \beta_7 x_{i7}$$ is linear for the covariates $x_n$

### ii) 


```{r}

#make predictions on test set
pred <- predict(logReg, newdata = test, type = "response")

#convert predictions to numeric values (0/1)
numpred <- as.numeric(pred>0.5)

#create reference on test set
reference <- test$diabetes

#ensure correct levels
u <- union(numpred, reference)
t <- table(factor(numpred, u), factor(reference, u))

#make confusion matrix (and sensitivity/specificity etc.)
confusionMatrix(t)$table
confusionMatrix(t)$byClass

```

## b)

(i)
$\pi_k$ is the prior probability that a random observation comes from the $k-th$ class. As we have two classes, where class 0 have 200 observations and class 1 have 100 observations from the training set, we obtain that $\pi_0 \approx 0.67$ and $\pi_1 \approx 0.33$

$\mu_k$ is the mean of $X$. It is a vector of size $p$, where $p$ is the number of predictors, and it can be estimated to 

$$
\hat{\mu_k}=\frac{1}{n_k}\sum_{i:y_i=k}{x_i}
$$


$\boldsymbol{\Sigma}$ is the $p\times p$-covariance matrix of $X$ which is common for all classes.

$f_k(x)$ is the multivariate Gaussian density, meaning the ...

### ii)

```{r}
#table(predict = predict(lda(diabetes ~ glu + bmi + ped, data = train))$class,  true = train$diabetes)


lda.fit = lda(diabetes ~ . , data = train)
lda.pred = predict(lda.fit, test, type= 'prob')
lda.table = table(predict = lda.pred$class, true = test$diabetes)
lda.table


qda.fit = qda(diabetes ~ ., data = train)
qda.pred = predict(qda.fit, test)
table(predict = qda.pred$class, true = test$diabetes)

```

## c)

### i)

A new observation $x_0$ is classified to the most occuring class of the K nearest nodes in the training data by Eucleidian distance. For K=1 $x_0$ is simply classified as the same class as the nearest node. 

### ii)

When choosing the tuning parameter $k$, one must consider the bias-variance trade off (small k leads to large bias, but low varaiance and vice verca). For this case we need a loss function and a validation set, which we will come back to. First, we split our data in to a test set, and a training set. Then, by applying cross validation to the training set, splitting up in training and validation sets, we are able to use the whole training set and still ensure a valid test set. The model is then fitted on the cross validated sets for different K, and a loss is computed for the validation set. The tuning parameter k is determined by evaluating for which K the valiation error is lowest. 

an other approach is to apply bootstrapping on the training set.

(iii)

```{r}


#traincl <- factor(diab[train, "classifications"])
knnMod = knn(train = train, test = test, cl = train$diabetes, k = 25, prob = T)
knnConfMat = table(knnMod, test$diabetes)

knnConfMat

sensitivity = knnConfMat[2,2]/(knnConfMat[2,2] + knnConfMat[1,2])
sensitivity

specificity = knnConfMat[1,1]/(knnConfMat[1,1] + knnConfMat[2,1])
specificity

```
### Problem 4








### Problem 5

Loading the bodyfat dataset given in the problem:
```{r}
id <- "19auu8YlUJJJUsZY8JZfsCTWzDm6doE7C" # google file ID
d.bodyfat <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id),header=T)
```
## a)
```{r}
r.bodyfat <- lm(bodyfat ~ . , data = d.bodyfat)

rsqrd <- summary(r.bodyfat)$r.squared

rsqrd

```

The $R^{2}$ is found to be 0.728.


## b)

### i) 
Generate 1000 bootstrap samples of the $R^{2}$



```{r}

N = length(d.bodyfat[,1]) #Finding length of bodyfat


set.seed(4268)
#boot(data = r.bodyfat, statistic = , R = 1000)
index <- sample(1:N, N, replace = TRUE)
  
newdata.d <- d.bodyfat[index, ]

B = 1000

r2stored <- rep(0, B) #Generating a list with B zeros

for (i in 1:B){
  index <- sample(1:N, N, replace = TRUE) #Generating randon integers from 1, length of dataset
  newbodyfat.d <- d.bodyfat[index, ] #creating a new dataset with the generated indexes
  bodyfat.boot <- lm(bodyfat ~ . , data = newbodyfat.d)#fitting the regression model
  r2stored[i] = summary(bodyfat.boot)$r.squared #appending r squared to the r2stored list.
  
}
```


### ii)
Plotting the distribution of the values of $R^{2}$

```{r}
hist(r2stored,
     main = "Distribution of R^2",
     xlab = "R^2",
     ylab = "Frequency" )
```



### iii)



```{r}
index <- sample(1:N, N, replace = TRUE)

mean.r2 = mean(r2stored)
SE = sqrt(1/(B-1)*sum((r2stored - mean.r2)^2))
confInverval = quantile(r2stored, probs = c(5, 95)/100)

SE
confInverval







```


 ### iv)
 
 








