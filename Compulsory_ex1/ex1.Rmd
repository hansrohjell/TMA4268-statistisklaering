---
subtitle: "TMA4268 Statistical Learning V2021"
title: "Compulsory exercise 1: Group 5"
author: "Hans RÃ¸hjell Odland and Aksel Haugen Madslien"
date: "2/9/2021"
output: 
  pdf_document
editor_options: 
  chunk_output_type: inline
---
  

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=4, fig.height=3)
library(tidyverse)
library(ggplot2)
library(caret)
library(boot)
library(class)
library(MASS)

```

```{r rpackages,eval=FALSE,echo=FALSE}
install.packages("knitr") #probably already installed
install.packages("rmarkdown") #probably already installed
install.packages("ggplot2") #plotting with ggplot
install.packages("ggfortify")  
install.packages("MASS")
install.packages("class")
install.packages("pROC")
install.packages("plotROC")
install.packages('boot',dep=TRUE)
install.packages('topicmodels')
install.packages('formatR')
```




For some problems you will need to include some LaTex code. Please install latex on your computer and then consult Compulsory1.Rmd for hints how to write formulas in LaTex. 

An example:

$$Y_i  = f(x_i) + \varepsilon_i \ ,$$

Or the same formula $$Y_i  = f(x_i) + \varepsilon_i$$ in-line.


# Problem 1

## a)
 We consider $$Y=f(\mathbf {x})+\varepsilon, \text{ where } \text{E}(\varepsilon)=0 \text{ and } \text{Var}(\varepsilon)=\sigma^2.$$
 
We find the expected value for $$\tilde{\beta}$$ as 


\begin{align}
  E(\tilde{\beta}) = E[(\mathbf{x}^T\mathbf{x}+\lambda \mathbf{I})^{-1}\mathbf{x}^T\mathbf{y}] \\
  &= (\mathbf{x}^T\mathbf{x}+\lambda \mathbf{I})^{-1} \mathbf{x}^T E[\mathbf{y}] \\
  &= (\mathbf{x}^T\mathbf{x}+\lambda \mathbf{I})^{-1} \mathbf{x}^T \mathbf{x} \beta + \lambda\mathbf{I}\beta - \lambda\mathbf{I}\beta \\
  &= (\mathbf{x}^T\mathbf{x}+\lambda \mathbf{I})^{-1} (-\lambda\mathbf{I})\beta + \mathbf{I}\beta \\
  &= \beta - \lambda(\mathbf{x}^T\mathbf{x}+\lambda \mathbf{I})\beta
\end{align}

## b)
 We let $$\widetilde{f}(\mathbf{x}_0)=\mathbf{x}_0^T \widetilde{\boldsymbol{\beta}}$$ The variance for $$\widetilde{f}(\mathbf{x}_0)$$ then becomes
 
 \begin{align}
 E[\widetilde{f}(\mathbf{x}_0)]=E[\mathbf{x}_0^T \widetilde{\boldsymbol{\beta}}] 
 &= \mathbf{x}_0^T E[\widetilde{\boldsymbol{\beta}}] \\
 &= \mathbf{x}_0^T(\beta - \lambda(\mathbf{x}^T\mathbf{x}+\lambda \mathbf{I})\beta)
 \end{align}
 
 
 For the variation we get 
 
 \begin{align}
 Var[\widetilde{f}(\mathbf{x}_0)] = \mathbf{x}_0^T Var[\widetilde{\boldsymbol{\beta}}]\mathbf{x} \\
 &= \mathbf{x}_0^T(\beta - \lambda(\mathbf{x}^T\mathbf{x}+\lambda \mathbf{I})\beta)\mathbf{x}
 \end{align}
 
 
## c)


## d) 

```{r, echo=TRUE, eval=TRUE}
id <- "1X_8OKcoYbng1XvYFDirxjEWr7LtpNr1m" # google file ID
values <- dget(sprintf("https://docs.google.com/uc?id=%s&export=download", id))
X = values$X
dim(X)
x0 = values$x0
dim(x0)
beta=values$beta
dim(beta)
sigma=values$sigma
sigma
```



```{r}

bias = function(lambda, X, x0, beta) {
    p = ncol(X)
    value = 1
    return(value)
}
lambdas = seq(0, 2, length.out = 500)
BIAS = rep(NA, length(lambdas))
for (i in 1:length(lambdas)) BIAS[i] = bias(lambdas[i], X, x0, beta)
dfBias = data.frame(lambdas = lambdas, bias = BIAS)
ggplot(dfBias, aes(x = lambdas, y = bias)) + geom_line(color = "red") + xlab(expression(lambda)) + 
    ylab(expression(bias^2))
```

 

# Problem 2

## a) 

```{r}
id <- "1yYlEl5gYY3BEtJ4d7KWaFGIOEweJIn__" # google file ID

d.corona <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id),header=T)


table(d.corona$deceased)

table(d.corona$country, d.corona$sex)

table(d.corona$sex, d.corona$deceased)

francedf <- subset(d.corona, country == "France")
table(francedf$sex, francedf$deceased)




```
## b) 

### i)
```{r}

# Multiple Linear Regression Example
fit <- lm(as.numeric(deceased) ~ sex + country + age, data=d.corona)
summary(fit)$coef # show results

#predict(fit)

deceasedmale <- fit$coefficients[6]*75 + fit$coefficients[2] + fit$coefficients[5]

deceasedmale



```

The probability of dying of Covid-19 for a male age in Korea is found to be $5.7\%$.


### ii)

Does males have higher probability to die than females?

```{r}

fit <- glm(deceased ~ sex, data = d.corona, family = binomial)
summary(fit)$coef


```

The estimate readings for men dying of corona is positive, which means that we can conclude that men have a higher probability of dying of corona than women.


###iii)

```{r}

fit <- glm(deceased ~ country, data = d.corona, family = 'binomial')
summary(fit)$coef

```
From these readings we can conclude that there is not enough evidence to say that there is a higher risk of dying of corona in Indonesia than in France, since the p-value is not significant. For Japan and Korea the p-value is much more significant, and also has a negative estimate, which means that there is a higher risk of dying of corona in Japan and Korea than in France.

### iv)
A person is 10 years older than another person. The probability of dying is linear in terms of age because of the logic regression, so we can see the probability of a person dying at an age of 65 and an age of 75 from task i and see that there is an increase of risk to die in case of higher age.

```{r}

deceasedmale75 <- fit$coefficients[6]*75 + fit$coefficients[2] + fit$coefficients[5]
deceasedmale65 <- fit$coefficients[6]*65 + fit$coefficients[2] + fit$coefficients[5]

diff <- (deceasedmale75 - deceasedmale65)*100

``` 

This gives an age difference in $1.3\%$ and we see that the model is linear.



## c)


### i)

```{r}
fit <- glm(deceased ~ age*sex, data = d.corona, family = 'binomial')
summary(fit)$coef

``` 

Here we see that the $`age:sexmale`$ coefficients has a positive estimate, but doesn't have a significant p-value. Age has a slightly lower p-value, and we can see that age is not a greater risk factor for males than for females.


### ii)


```{r}
fit <- glm(deceased ~ age*country, data = d.corona, family = 'binomial')
summary(fit)$coef


```
No, the coefficient for the $`age:countryindonesia`$ interaction is negative, which means that Indonesia is lower than it is for France. The p-value is slightly significant, which gives a low but greater risk factor for the Indonesian population than for the French.


## d)

```{r}

fit <- glm(deceased ~ . ,data = d.corona , family = 'binomial')
summary(fit)


table(predict = predict(lda(deceased ~ age + sex + country, data = d.corona))$class, true = d.corona$deceased)


table(predict = predict(qda(deceased ~ age + sex + country, data = d.corona))$class, true = d.corona$deceased)

```
FALSE, TRUE, TRUE, FALSE


### Problem 3

## a)

```{r, eval=T}
#read file
id <- "1i1cQPeoLLC_FyAH0nnqCnnrSBpn05_hO" # google file ID
diab <- dget(sprintf("https://docs.google.com/uc?id=%s&export=download", id))
t = MASS::Pima.tr2
train = diab$ctrain
test = diab$ctest
```

```{r}
logReg = glm(diabetes~., data = train, family = "binomial")
summary(logReg)
```
## (i)
prove that logit is linear
By denoting the term $\beta_0 + \beta_1x_{i1} + \beta_2 x_{i2} + \dots + \beta_7 x_{i7}$ as $\beta_{sum}$, we can derive that


\begin{align}
\text{logit}(p_i) = \log(\frac{p_i}{1-p_i})
= \log(\frac{\frac{e^{\beta_{sum}}}{ 1+ 
e^{\beta_{sum}}}}{1-\frac{e^{\beta_{sum}}}{ 1+ 
e^{\beta_{sum}}}}) \\
&= \log(\frac{\frac{e^{\beta_{sum}}}{ 1+ 
e^{\beta_{sum}}}}{\frac{1+e^{\beta_{sum}}}{ 1+ 
e^{\beta_{sum}}}-\frac{e^{\beta_{sum}}}{ 1+ 
e^{\beta_{sum}}}}) \\
&= \log(\frac{\frac{e^{\beta_{sum}}}{ 1+ 
e^{\beta_{sum}}}}{\frac{1+e^{\beta_{sum}} - e^{\beta_{sum}}}{ 1+ 
e^{\beta_{sum}}}}) \\
&= \log(\frac{e^{\beta_{sum}}}{1+e^{\beta_{sum}} - e^{\beta_{sum}}})
&= \beta_{sum}
\end{align}

As we know, $$\beta_{sum}=\beta_0 + \beta_1x_{i1} + \beta_2 x_{i2} + \dots + \beta_7 x_{i7}$$ is linear for the covariates $x_n$

(ii) 


```{r}


#install package e1071 (required to ensure correct levels in given manner)


#make predictions on test set
pred <- predict(logReg, newdata = test, type = "response")

#convert predictions to numeric values (0/1)
numpred <- as.numeric(pred>0.5)

#create reference on test set
reference <- test$diabetes

#ensure correct levels
u <- union(numpred, reference)
t <- table(factor(numpred, u), factor(reference, u))

#make confusion matrix (and sensitivity/specificity etc.)
confusionMatrix(t)



```

## b

(i)
$\pi_k$ is the prior probability that a random observation comes from the $k-th$ class. As we have two classes, where class 0 have 200 observations and class 1 have 100 observations from the training set, we obtain that $\pi_0 \approx 0.67$ and $\pi_1 \approx 0.33$

$\mu_k$ is the mean of $X$. It is a vector of size $p$, where $p$ is the number of predictors, and it can be estimated to 

$$
\hat{\mu_k}=\frac{1}{n_k}\sum_{i:y_i=k}{x_i}
$$


$\boldsymbol{\Sigma}$ is the $p\times p$-covariance matrix of $X$ which is common for all classes.

$f_k(x)$ is the multivariate Gaussian density, meaning the ...

### ii)

```{r}
#table(predict = predict(lda(diabetes ~ glu + bmi + ped, data = train))$class,  true = train$diabetes)


lda.fit = lda(diabetes ~ . , data = train)
lda.pred = predict(lda.fit, test, type= 'prob')
lda.table = table(predict = lda.pred$class, true = test$diabetes)
lda.table


qda.fit = qda(diabetes ~ ., data = train)
qda.pred = predict(qda.fit, test)
qda.table = table(predict = qda.pred$class, true = test$diabetes)
qda.table
```

##c

(i)

A new observation $x_0$ is classified to the most occuring class of the K nearest nodes in the training data by Eucleidian distance. For K=1 $x_0$ is simply classified as the same class as the nearest node. 

(ii)

When choosing the tuning parameter k, one must consider the bias-variance trade off (small k leads to large bias, but low varaiance and vice verca). For this case we need a loss function and a validation set, which we will come back to. First, we split our data in to a test set, and a training set. Then, by applying cross validation to the training set, splitting up in training and validation sets, we are able to use the whole training set and still ensure a valid test set. The model is then fitted on the cross validated sets for different K, and a loss is computed for the validation set. The tuning parameter k is determined by evaluating for which K the valiation error is lowest. 

an other approach is to apply bootstrapping on the training set.

(iii)

```{r}


#traincl <- factor(diab[train, "classifications"])
knnMod = knn(train = train, test = test, cl = train$diabetes, k = 25, prob = T)
knnConfMat = table(knnMod, test$diabetes)

knnConfMat

sensitivity = knnConfMat[2,2]/(knnConfMat[2,2] + knnConfMat[1,2])
sensitivity

specificity = knnConfMat[1,1]/(knnConfMat[1,1] + knnConfMat[2,1])
specificity

```
### Problem 4








### Problem 5

Loading the bodyfat dataset given in the problem:
```{r}
id <- "19auu8YlUJJJUsZY8JZfsCTWzDm6doE7C" # google file ID
d.bodyfat <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id),header=T)
```
## a)
```{r}
r.bodyfat <- lm(bodyfat ~ . , data = d.bodyfat)

rsqrd <- summary(r.bodyfat)$r.squared

rsqrd

```

The $R^{2}$ is found to be 0.728.


## b)

### i) 
Generate 1000 bootstrap samples of the $R^{2}$

```{r}

N = length(d.bodyfat[,1])


set.seed(4268)
#boot(data = r.bodyfat, statistic = , R = 1000)
index <- sample(1:N, N, replace = TRUE)

newdata.d <- d.bodyfat[index, ]

B = 1000

for (i in i:B){
  
}






```















